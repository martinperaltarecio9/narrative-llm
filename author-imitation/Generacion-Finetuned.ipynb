{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a559f5d",
   "metadata": {},
   "source": [
    "# Generación del dataset para hacer inferencia con el Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310e720",
   "metadata": {},
   "source": [
    "Este notebook genera textos sintéticos estilo Shakespeare utilizando el modelo\n",
    "Mistral 7B fine-tuneado con LoRA.\n",
    "\n",
    "Se generan múltiples textos combinando distintos prompts, con el objetivo de construir un dataset de benchmark que luego se utiliza para evaluación automática (clasificador, perplexity y métricas finales)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1fdde1",
   "metadata": {},
   "source": [
    "## 1. Imports y rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL_DIR = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-7b-instruct-v0.3\"\n",
    "LORA_DIR       = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-finetuneado(lora)\"\n",
    "OUTPUT_DIR     = \"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data/mistral_finetune\"\n",
    "\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generación\n",
    "N_SAMPLES_PER_COMBO = 20\n",
    "MAX_NEW_TOKENS = 700\n",
    "BASE_SEED = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b26028",
   "metadata": {},
   "source": [
    "## 2. Prompts de generación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_PROMPT = \"\"\"\n",
    "Write a single paragraph between 150 and 300 words in the style of\n",
    "Shakespeare's stories. The paragraph must be original,\n",
    "not copied, and self-contained.\n",
    "\"\"\"\n",
    "\n",
    "BETTER_PROMPT = \"\"\"\n",
    "You are an expert writer imitating William Shakespeare.\n",
    "\n",
    "Write one single self-contained paragraph between 150 and 300 words in Early Modern English,\n",
    "in the style of Shakespeare’s plays and sonnets. The paragraph must be original, not copied,\n",
    "and should use iambic or quasi-iambic rhythm, archaic pronouns (thee, thou, thy), and\n",
    "elevated metaphors.\n",
    "\n",
    "Avoid copying any real Shakespeare sentences; the text must be entirely new.\n",
    "\"\"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"basic\": BASIC_PROMPT,\n",
    "    \"better\": BETTER_PROMPT,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ffa0e8",
   "metadata": {},
   "source": [
    "## 3. Carga del tokenizer y modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514744f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "# Modelo base\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_DIR,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "# Modelo con LoRA\n",
    "model_lora = PeftModel.from_pretrained(model_base, LORA_DIR)\n",
    "model_lora.eval()\n",
    "\n",
    "MODELS = {\n",
    "    \"lora\": model_lora,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca05a1",
   "metadata": {},
   "source": [
    "## 4. Función de generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77759c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_new_tokens=700, seed=None):\n",
    "    \"\"\"\n",
    "    Genera texto a partir de un prompt usando sampling.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e40fd",
   "metadata": {},
   "source": [
    "## 5. Generación y guardado de textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c657430",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in MODELS.items():\n",
    "    for prompt_name, prompt in PROMPTS.items():\n",
    "        print(f\"=== Modelo: {model_name} | Prompt: {prompt_name} ===\")\n",
    "\n",
    "        for i in range(N_SAMPLES_PER_COMBO):\n",
    "            seed = BASE_SEED + i\n",
    "            text = generate_text(\n",
    "                model,\n",
    "                prompt,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "            filename = f\"{model_name}_{prompt_name}_{i:02d}.txt\"\n",
    "            out_path = OUTPUT_DIR / filename\n",
    "\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            print(\"Guardado:\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4db6b1",
   "metadata": {},
   "source": [
    "## 6. Limpieza del prompt (en caso de que suceda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8295483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prompt_prefix(text: str, prompt: str) -> str:\n",
    "    if text.startswith(prompt):\n",
    "        return text[len(prompt):].strip()\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "for path in OUTPUT_DIR.glob(\"*.txt\"):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Intentar limpiar ambos prompts\n",
    "    cleaned = remove_prompt_prefix(text, BASIC_PROMPT)\n",
    "    cleaned = remove_prompt_prefix(cleaned, BETTER_PROMPT)\n",
    "\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
