{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b36863e0",
   "metadata": {},
   "source": [
    "# LLM - Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ee00d",
   "metadata": {},
   "source": [
    "Objetivo del notebook: realizar un finetune del modelo Mistral 7B usando LoRA sobre el corpus separado de Shakespeare.\n",
    "Este notebook fue pensado para ser ejecutado desde Google Colab, de ahí viene la configuración del entorno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16464e",
   "metadata": {},
   "source": [
    "## 0. Setup del entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "!pip install -q \\\n",
    "  \"transformers>=4.45.0\" \\\n",
    "  \"datasets>=3.0.0\" \\\n",
    "  \"accelerate>=1.0.0\" \\\n",
    "  \"peft>=0.13.0\" \\\n",
    "  \"trl>=0.9.0\" \\\n",
    "  bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93f1e82",
   "metadata": {},
   "source": [
    "## 1. Imports y configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4372739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aab674",
   "metadata": {},
   "source": [
    "## 2. Paths y carga del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82af199",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DIR = \"/content/drive/MyDrive/StoryWriter/Data/Training_data/redactor_train\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-7b-instruct-v0.3\"\n",
    "\n",
    "paths = sorted(Path(TEXT_DIR).glob(\"*.txt\"))\n",
    "print(\"Archivos encontrados:\", len(paths))\n",
    "\n",
    "records = []\n",
    "for p in paths:\n",
    "    text = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    if text:\n",
    "        records.append({\"text\": text})\n",
    "\n",
    "dataset = Dataset.from_list(records)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee0653b",
   "metadata": {},
   "source": [
    "## 3. Tokenizer y modelo base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf942ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.model_max_length = 1024\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Modelo base cargado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194169a9",
   "metadata": {},
   "source": [
    "## 4. Configuración LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c437894",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12fa54",
   "metadata": {},
   "source": [
    "## 5. Configuración del entrenamiento (TRL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7848e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = SFTConfig(\n",
    "    output_dir=\"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-lora\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=torch.cuda.is_available(),\n",
    "    packing=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_config,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed20e4",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento y guardado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Guardar adapters LoRA\n",
    "trainer.model.save_pretrained(\n",
    "    \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-lora\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
