{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5f9618",
   "metadata": {},
   "source": [
    "# Parseado de datos para Discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e25bd0",
   "metadata": {},
   "source": [
    "Este notebook genera textos de control negativo (no Shakespeare) a partir de obras literarias de dominio público obtenidas de Project Gutenberg. <br>\n",
    "Los textos se limpian, segmentan en chunks de longitud comparable a los textos generados por modelos, y se guardan como archivos .txt para ser utilizados en el entrenamiento y evaluación de un clasificador estilístico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21c71c",
   "metadata": {},
   "source": [
    "## 1. Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85795449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc9f79c",
   "metadata": {},
   "source": [
    "## 2. Configuración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b4d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios de trabajo (pensado para Google Colab)\n",
    "RAW_DIR = Path(\"/content/raw_fino\")\n",
    "OUTPUT_DIR = Path(\n",
    "    \"/content/drive/MyDrive/StoryWriter/Data/Training_data/\"\n",
    "    \"clasificator_train/negativos_finos\"\n",
    ")\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Parámetros de longitud de texto\n",
    "MIN_WORDS = 150\n",
    "MAX_WORDS = 400\n",
    "\n",
    "# Máximo de chunks por libro (para balancear el dataset)\n",
    "MAX_CHUNKS_PER_BOOK = 100\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2116a",
   "metadata": {},
   "source": [
    "## 3. Descarga de libros (control negativo grueso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fe196",
   "metadata": {},
   "outputs": [],
   "source": [
    "GUTENBERG_URLS = [\n",
    "    # Aventura / General\n",
    "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",  # Moby Dick\n",
    "    \"https://www.gutenberg.org/cache/epub/74/pg74.txt\",     # Tom Sawyer\n",
    "    \"https://www.gutenberg.org/cache/epub/55/pg55.txt\",     # Wizard of Oz\n",
    "\n",
    "    # Ciencia ficción\n",
    "    \"https://www.gutenberg.org/cache/epub/18857/pg18857.txt\",  # Journey to the Centre\n",
    "    \"https://www.gutenberg.org/cache/epub/35/pg35.txt\",       # The Time Machine\n",
    "    \"https://www.gutenberg.org/cache/epub/36/pg36.txt\",       # War of the Worlds\n",
    "\n",
    "    # Fantasía / Romance\n",
    "    \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",    # Alice\n",
    "    \"https://www.gutenberg.org/cache/epub/45/pg45.txt\",    # Anne of Green Gables\n",
    "    \"https://www.gutenberg.org/cache/epub/16/pg16.txt\",    # Peter Pan\n",
    "\n",
    "    # Terror / Drama\n",
    "    \"https://www.gutenberg.org/cache/epub/345/pg345.txt\",  # Dracula\n",
    "    \"https://www.gutenberg.org/cache/epub/84/pg84.txt\",    # Frankenstein\n",
    "    \"https://www.gutenberg.org/cache/epub/43/pg43.txt\",    # Jekyll & Hyde\n",
    "]\n",
    "\n",
    "for url in GUTENBERG_URLS:\n",
    "    fname = url.split(\"/\")[-1]\n",
    "    out_path = RAW_DIR / fname\n",
    "    if out_path.exists():\n",
    "        print(\"Ya existe:\", out_path.name)\n",
    "        continue\n",
    "    !wget -q \"{url}\" -O \"{out_path}\"\n",
    "    print(\"Descargado:\", out_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762df01",
   "metadata": {},
   "source": [
    "## 4. Definición de funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac584bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gutenberg_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpieza mínima del texto:\n",
    "    - normaliza saltos de línea\n",
    "    - elimina espacios duplicados\n",
    "    - colapsa bloques grandes de líneas vacías\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_paragraphs(text: str):\n",
    "    \"\"\"\n",
    "    Divide el texto en párrafos usando dobles saltos de línea.\n",
    "    \"\"\"\n",
    "    return [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "\n",
    "def make_chunks_from_paragraphs(paragraphs, min_words=MIN_WORDS, max_words=MAX_WORDS):\n",
    "    \"\"\"\n",
    "    Junta párrafos hasta alcanzar un rango de palabras [min_words, max_words].\n",
    "    Devuelve una lista de strings (chunks).\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "    current_len = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        words = para.split()\n",
    "        if current_len + len(words) > max_words and current_words:\n",
    "            if current_len >= min_words:\n",
    "                chunks.append(\" \".join(current_words))\n",
    "            current_words = []\n",
    "            current_len = 0\n",
    "\n",
    "        current_words.extend(words)\n",
    "        current_len += len(words)\n",
    "\n",
    "    if current_words and current_len >= min_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09c3d8",
   "metadata": {},
   "source": [
    "## 5. Procesamiento completo del corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_books(input_dir: Path):\n",
    "    \"\"\"\n",
    "    Procesa todos los libros del directorio:\n",
    "    - limpia texto\n",
    "    - divide en párrafos\n",
    "    - genera chunks\n",
    "    - selecciona un subconjunto aleatorio\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for path in sorted(input_dir.glob(\"*.txt\")):\n",
    "        print(\"Procesando:\", path.name)\n",
    "        text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        clean = clean_gutenberg_text(text)\n",
    "\n",
    "        paragraphs = split_into_paragraphs(clean)\n",
    "        chunks = make_chunks_from_paragraphs(paragraphs)\n",
    "\n",
    "        random.shuffle(chunks)\n",
    "        selected = chunks[:MAX_CHUNKS_PER_BOOK]\n",
    "        all_chunks.extend(selected)\n",
    "\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "chunks = process_books(RAW_DIR)\n",
    "print(f\"Total de chunks generados: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91ecf1",
   "metadata": {},
   "source": [
    "## 6. Guardado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a150153",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    out_path = OUTPUT_DIR / f\"chunk_{i:05d}.txt\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(chunk)\n",
    "\n",
    "print(f\"Guardados {len(chunks)} archivos en:\")\n",
    "print(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
