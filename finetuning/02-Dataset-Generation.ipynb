{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeDA--B2Ool9",
        "outputId": "bbcf23e7-2fdb-4c08-c831-3f9645db72ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes transformers torch datasets openai requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq6jl2IF-NYJ"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Script para recolección de datos de benchmark estilo Shakespeare\n",
        "# Pensado para ejecutarse en Google Colab\n",
        "# Requiere:\n",
        "#   pip install transformers torch datasets openai requests\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import textwrap\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from openai import OpenAI\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "#from trl import SFTTrainer, SFTConfig\n",
        "from peft import LoraConfig, get_peft_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "3cqpt6QO-SFp",
        "outputId": "e80aff69-3c9b-4c72-ab58-8edac28e5fdb"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'mkdir'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-378678253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#    BENCH_DIR / \"neg_fine\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m ]:\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Longitud objetivo de párrafos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'mkdir'"
          ]
        }
      ],
      "source": [
        "# ----------------------------\n",
        "# CONFIGURACIÓN GLOBAL\n",
        "# ----------------------------\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data\"\n",
        "\n",
        "# Crear todas las carpetas necesarias\n",
        "for p in [\n",
        "#    BENCH_DIR / \"Shakespeare_like_finetuned\",\n",
        "    BASE_DIR + \"Shakespeare_like_gpt\",\n",
        "#    BENCH_DIR / \"neg_coarse\",\n",
        "#    BENCH_DIR / \"neg_fine\",\n",
        "]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Longitud objetivo de párrafos\n",
        "MIN_WORDS = 150\n",
        "MAX_WORDS = 300\n",
        "\n",
        "# Split de controles negativos:\n",
        "# proporción que va al benchmark vs al set de entrenamiento de Roberta\n",
        "#NEG_BENCH_FRAC = 0.10  # 10% benchmark, 90% entrenamiento\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jMSG4pfzTgf"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-7b-instruct-v0.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-_5fpdP0tG8"
      },
      "outputs": [],
      "source": [
        "import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "QNGM-wxUzLNw",
        "outputId": "a1aa0e76-34c4-46a9-8240-6410caa85035"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3342767622.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mMODEL_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4879\u001b[0m                 )\n\u001b[1;32m   4880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4881\u001b[0;31m         hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n\u001b[0m\u001b[1;32m   4882\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4883\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py\u001b[0m in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_library_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             )\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tokenizer.model_max_length = 512\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    quantization_config=bnb_config,\n",
        "    dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRa3Cer7PZsC"
      },
      "outputs": [],
      "source": [
        "MIN_WORDS = 150\n",
        "MAX_WORDS = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP41Givl-WgA"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 1. HELPERS GENERALES\n",
        "# ----------------------------\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Limpieza mínima de texto.\"\"\"\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def count_words(text: str) -> int:\n",
        "    return len(re.findall(r\"\\w+\", text))\n",
        "\n",
        "def cut_to_word_range(text: str, min_w=MIN_WORDS, max_w=MAX_WORDS) -> str | None:\n",
        "    \"\"\"\n",
        "    Toma un texto y devuelve un sub-párrafo entre min_w y max_w palabras.\n",
        "    Si no encuentra, devuelve None.\n",
        "    Estrategia simple: tomar la ventana inicial de tamaño max_w\n",
        "    y recortar al punto final más cercano.\n",
        "    \"\"\"\n",
        "    words = re.findall(r\"\\S+\", text)\n",
        "    if len(words) < min_w:\n",
        "        return None\n",
        "\n",
        "    if len(words) <= max_w:\n",
        "        return \" \".join(words)\n",
        "\n",
        "    # Tomamos un slice de tamaño max_w\n",
        "    slice_words = words[:max_w]\n",
        "    # Intentar cortar en el último punto\n",
        "    joined = \" \".join(slice_words)\n",
        "    last_dot = joined.rfind(\".\")\n",
        "    if last_dot != -1 and count_words(joined[:last_dot]) >= min_w:\n",
        "        return joined[:last_dot+1].strip()\n",
        "    # Si no, devolver las primeras max_w palabras sin más\n",
        "    return joined.strip()\n",
        "\n",
        "def save_paragraphs(paragraphs, out_dir: Path, prefix: str):\n",
        "    \"\"\"\n",
        "    Guarda cada párrafo como un .txt en out_dir, con nombre prefix_XXXX.txt\n",
        "    Devuelve la lista de paths escritos.\n",
        "    \"\"\"\n",
        "    out_paths = []\n",
        "    for i, text in enumerate(paragraphs):\n",
        "        path = out_dir / f\"{prefix}_{i:05d}.txt\"\n",
        "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(text)\n",
        "        out_paths.append(path)\n",
        "    return out_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqHtahR4VorG"
      },
      "outputs": [],
      "source": [
        "    \"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKvyru7wVpuv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "You are an expert writer imitating William Shakespeare.\n",
        "\n",
        "Write one single self-contained paragraph between 150 and 300 words in Early Modern English,\n",
        "in the style of Shakespeare’s plays and sonnets. The paragraph must be original, not copied,\n",
        "and should use iambic or quasi-iambic rhythm, archaic pronouns (thee, thou, thy), and\n",
        "elevated metaphors.\n",
        "\n",
        "Avoid copying any real Shakespeare sentences; the text must be entirely new.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTfn-GBuPe8R"
      },
      "outputs": [],
      "source": [
        "PROMPT_PRO = \"\"\"\n",
        "You are an expert writer imitating William Shakespeare.\n",
        "\n",
        "Write one single self-contained paragraph between 150 and 300 words in Early Modern English,\n",
        "in the style of Shakespeare’s plays and sonnets. The paragraph must be original, not copied,\n",
        "and should use iambic or quasi-iambic rhythm, archaic pronouns (thee, thou, thy), and\n",
        "elevated metaphors.\n",
        "\n",
        "Avoid copying any real Shakespeare sentences; the text must be entirely new.\n",
        "\"\"\"\n",
        "\n",
        "PROMPT =     \"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\"\n",
        "\n",
        "def generate_Shakespeare_like_gpt(n_samples: int, prompt) -> list[str]:\n",
        "    paragraphs = []\n",
        "    for i in range(n_samples):\n",
        "        print(f\"[HF-GPT] Generando párrafo {i+1}/{n_samples} ...\")\n",
        "        out = gpt_pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=450,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "        )[0][\"generated_text\"]\n",
        "\n",
        "        # muchas veces el modelo devuelve prompt + continuación\n",
        "        text = out[len(PROMPT_BASE_Shakespeare):].strip()\n",
        "        text = clean_text(text)\n",
        "        text = cut_to_word_range(text)\n",
        "        if text is None:\n",
        "            continue\n",
        "        paragraphs.append(text)\n",
        "    return paragraphs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "PvRz-OIPPi0M",
        "outputId": "11497794-c4de-4ecd-a0dd-793c143d2c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HF-GPT] Generando párrafo 1/1 ...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'gpt_pipe' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-902843029.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_Shakespeare_like_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPROMPT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3476759862.py\u001b[0m in \u001b[0;36mgenerate_Shakespeare_like_gpt\u001b[0;34m(n_samples, prompt)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[HF-GPT] Generando párrafo {i+1}/{n_samples} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         out = gpt_pipe(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gpt_pipe' is not defined"
          ]
        }
      ],
      "source": [
        "pars = generate_Shakespeare_like_gpt(1, PROMPT)\n",
        "print(pars[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwJxNHOqRL1S",
        "outputId": "e3b56c7e-e509-4374-b37e-c2c6e103f9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 14 validation chunks\n"
          ]
        }
      ],
      "source": [
        "for i, chunk in enumerate(pars):\n",
        "    file_path = os.path.join(\"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data/mistral_base\", f\"chunk_{i:04d}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(chunk)\n",
        "print(f\"Saved {len(pars)} validation chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3GSyVcIC25Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyVKsuIe-a0u"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 2. GPT PARA GENERAR PÁRRAFOS TIPO Shakespeare\n",
        "# ----------------------------\n",
        "\n",
        "# IMPORTANTE: Configurá tu API key en el entorno de Colab:\n",
        "#   import os\n",
        "#   os.environ[\"OPENAI_API_KEY\"] = \"tu_api_key\"\n",
        "client = OpenAI()\n",
        "\n",
        "GPT_MODEL = \"gpt-4.1-mini\"  # o el que quieras usar para generación\n",
        "\n",
        "PROMPT_BASE_Shakespeare = (\n",
        "    \"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "def generate_Shakespeare_like_gpt(n_samples: int) -> list[str]:\n",
        "    paragraphs = []\n",
        "    for i in range(n_samples):\n",
        "        print(f\"[GPT] Generando párrafo {i+1}/{n_samples} ...\")\n",
        "        resp = client.chat.completions.create(\n",
        "            model=GPT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": PROMPT_BASE_Shakespeare}],\n",
        "            temperature=0.9,\n",
        "            max_tokens=450,  # suficiente para 300 palabras aprox\n",
        "        )\n",
        "        text = resp.choices[0].message.content\n",
        "        text = clean_text(text)\n",
        "        text = cut_to_word_range(text)\n",
        "        if text is None:\n",
        "            continue\n",
        "        paragraphs.append(text)\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "u0Li9PxtIRXd",
        "outputId": "d8f1be73-45fa-4ecb-cbc7-3ff031886b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GPT] Generando párrafo 1/1 ...\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3552361788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_Shakespeare_like_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-90378718.py\u001b[0m in \u001b[0;36mgenerate_Shakespeare_like_gpt\u001b[0;34m(n_samples)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[GPT] Generando párrafo {i+1}/{n_samples} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPT_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPROMPT_BASE_Shakespeare\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1188\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "generate_Shakespeare_like_gpt(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTaNk10Y-fcH"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 3. MODELO REDACTOR FINETUNEA­DO (LOCAL)\n",
        "# ----------------------------\n",
        "\n",
        "# Configurá acá la ruta o nombre HF de tu modelo finetuneado\n",
        "AUTHOR_MODEL_NAME = \"ruta/o/nombre/de/tu/modelo_finetuneado\"\n",
        "\n",
        "def load_author_model():\n",
        "    device = 0 if torch.cuda.is_available() else -1\n",
        "    tok = AutoTokenizer.from_pretrained(AUTHOR_MODEL_NAME)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(AUTHOR_MODEL_NAME)\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mdl,\n",
        "        tokenizer=tok,\n",
        "        device=device,\n",
        "    )\n",
        "    return gen_pipe\n",
        "\n",
        "PROMPT_REDACTOR_BASE = (\n",
        "    \"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\"\"\n",
        ")\n",
        "\n",
        "def generate_Shakespeare_like_finetuned(n_samples: int, gen_pipe) -> list[str]:\n",
        "    paragraphs = []\n",
        "    for i in range(n_samples):\n",
        "        print(f\"[FINETUNED] Generando párrafo {i+1}/{n_samples} ...\")\n",
        "        out = gen_pipe(\n",
        "            PROMPT_REDACTOR_BASE,\n",
        "            max_new_tokens=450,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95,\n",
        "            num_return_sequences=1,\n",
        "        )[0][\"generated_text\"]\n",
        "        # quitar el prompt si quedó\n",
        "        text = out[len(PROMPT_REDACTOR_BASE):].strip()\n",
        "        text = clean_text(text)\n",
        "        text = cut_to_word_range(text)\n",
        "        if text is None:\n",
        "            continue\n",
        "        paragraphs.append(text)\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiO1W3gI-k8M"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 4. DESCARGA Y PARSING DE LIBROS (GUTENBERG O TXT LOCAL)\n",
        "# ----------------------------\n",
        "\n",
        "def download_gutenberg_book(gutenberg_id: int) -> str:\n",
        "    \"\"\"\n",
        "    Descarga libro de Project Gutenberg en texto plano.\n",
        "    NOTA: revisá siempre el estado de copyright según tu país.\n",
        "    \"\"\"\n",
        "    url = f\"https://www.gutenberg.org/files/{gutenberg_id}/{gutenberg_id}-0.txt\"\n",
        "    r = requests.get(url)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "def load_or_download_book(path_or_id: str | int) -> str:\n",
        "    \"\"\"\n",
        "    Si path_or_id es un path a .txt existente, lo lee.\n",
        "    Si es un int, asume Gutenberg ID y lo descarga.\n",
        "    \"\"\"\n",
        "    if isinstance(path_or_id, int):\n",
        "        text = download_gutenberg_book(path_or_id)\n",
        "    else:\n",
        "        path = Path(path_or_id)\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "    return clean_text(text)\n",
        "\n",
        "def split_book_into_paragraphs(book_text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Divide un libro en párrafos simples usando doble salto de línea.\n",
        "    Luego filtra por longitud y devuelve sólo los que están en [MIN, MAX] palabras.\n",
        "    \"\"\"\n",
        "    raw_pars = re.split(r\"\\n{2,}\", book_text)\n",
        "    pars = []\n",
        "    for p in raw_pars:\n",
        "        p = p.strip()\n",
        "        if not p:\n",
        "            continue\n",
        "        cut = cut_to_word_range(p)\n",
        "        if cut is not None:\n",
        "            pars.append(cut)\n",
        "    return pars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deWxPycP-pb5"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 5. RECOLECCIÓN DE PÁRRAFOS DE DIFERENTES FUENTES\n",
        "# ----------------------------\n",
        "\n",
        "def collect_negative_controls(neg_sources: list[str | int],\n",
        "                              out_bench_dir: Path,\n",
        "                              out_roberta_dir: Path,\n",
        "                              prefix_bench: str,\n",
        "                              prefix_roberta: str,\n",
        "                              max_paragraphs_per_book=200):\n",
        "    \"\"\"\n",
        "    neg_sources: libros para controles negativos (coarse o fine).\n",
        "    Split 90% train (Roberta) / 10% benchmark (configurable).\n",
        "    \"\"\"\n",
        "    all_pars = []\n",
        "    for src in neg_sources:\n",
        "        print(f\"[NEG] Procesando fuente {src} ...\")\n",
        "        text = load_or_download_book(src)\n",
        "        pars = split_book_into_paragraphs(text)\n",
        "        random.shuffle(pars)\n",
        "        all_pars.extend(pars[:max_paragraphs_per_book])\n",
        "\n",
        "    random.shuffle(all_pars)\n",
        "    n_total = len(all_pars)\n",
        "    n_bench = int(NEG_BENCH_FRAC * n_total)\n",
        "    bench_pars = all_pars[:n_bench]\n",
        "    roberta_pars = all_pars[n_bench:]\n",
        "\n",
        "    print(f\"Total párrafos neg: {n_total} -> benchmark {len(bench_pars)}, roberta {len(roberta_pars)}\")\n",
        "\n",
        "    save_paragraphs(bench_pars, out_bench_dir, prefix_bench)\n",
        "    save_paragraphs(roberta_pars, out_roberta_dir, prefix_roberta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJGD_Nu6-uXH"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 6. PIPELINE PRINCIPAL\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "# 6.1 Fuentes de libros (completá esto a mano)\n",
        "# ====================================================\n",
        "\n",
        "# Control negativo GRUESO: otros géneros (terror, aventura, drama, etc.)\n",
        "NEG_COARSE_SOURCES = [\n",
        "    # 766,  # EJEMPLO: Dracula (chequear ID real)\n",
        "    # \"poe_tales.txt\",\n",
        "]\n",
        "\n",
        "# Control negativo FINO: autores de misterio/policial NO Shakespeare\n",
        "NEG_FINE_SOURCES = [\n",
        "    # \"christie_book1.txt\",\n",
        "    # \"chesterton_father_brown.txt\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wHtEKQk-z32"
      },
      "outputs": [],
      "source": [
        "# ====================================================\n",
        "# 6.2 Recolección de libros\n",
        "# ====================================================\n",
        "\n",
        "collect_real_Shakespeare(Shakespeare_SOURCES)\n",
        "\n",
        "collect_negative_controls(\n",
        "    NEG_COARSE_SOURCES,\n",
        "    out_bench_dir=BENCH_DIR / \"neg_coarse\",\n",
        "    out_roberta_dir=ROBERTA_DIR / \"neg_coarse\",\n",
        "    prefix_bench=\"neg_coarse\",\n",
        "    prefix_roberta=\"neg_coarse\",\n",
        ")\n",
        "\n",
        "collect_negative_controls(\n",
        "    NEG_FINE_SOURCES,\n",
        "    out_bench_dir=BENCH_DIR / \"neg_fine\",\n",
        "    out_roberta_dir=ROBERTA_DIR / \"neg_fine\",\n",
        "    prefix_bench=\"neg_fine\",\n",
        "    prefix_roberta=\"neg_fine\",\n",
        ")\n",
        "\n",
        "# ====================================================\n",
        "# 6.3 Generación con GPT (tipo Shakespeare)\n",
        "# ====================================================\n",
        "\n",
        "N_GPT_SAMPLES = 200  # definí cuántos querés\n",
        "gpt_pars = generate_Shakespeare_like_gpt(N_GPT_SAMPLES)\n",
        "save_paragraphs(gpt_pars, BENCH_DIR / \"Shakespeare_like_gpt\", \"gpt_Shakespeare_like\")\n",
        "\n",
        "# ====================================================\n",
        "# 6.4 Generación con tu modelo REDACTOR finetuneado\n",
        "# ====================================================\n",
        "\n",
        "N_FINETUNED_SAMPLES = 200\n",
        "print(\"Cargando modelo finetuneado...\")\n",
        "gen_pipe = load_author_model()\n",
        "finetuned_pars = generate_Shakespeare_like_finetuned(N_FINETUNED_SAMPLES, gen_pipe)\n",
        "save_paragraphs(finetuned_pars, BENCH_DIR / \"Shakespeare_like_finetuned\", \"ft_Shakespeare_like\")\n",
        "\n",
        "print(\"✅ Recolección terminada.\")\n",
        "print(f\"Estructura de carpetas en: {BASE_DIR.resolve()}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
