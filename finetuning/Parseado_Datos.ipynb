{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. IMPORTS BÁSICOS\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "import random\n"
      ],
      "metadata": {
        "id": "J8lw2rySmvWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 2. DESCARGAR CORPUS DE shakespeare (INGLÉS)\n",
        "#    (desde Project Gutenberg, dominio público)\n",
        "# ============================================================\n",
        "\n",
        "os.makedirs(\"/content/raw_grueso\", exist_ok=True)\n",
        "\n",
        "# Todas las obras de shakespeare\n",
        "GUTENBERG_URLS = [\n",
        "  # AVENTURA\n",
        "    # 1. Moby Dick\n",
        "    \"https://www.gutenberg.org/cache/epub/2701/pg2701.txt\",\n",
        "    # 2. The Adventures of Tom Sawyer\n",
        "    \"https://www.gutenberg.org/cache/epub/74/pg74.txt\",\n",
        "    # 3. The Wonderful Wizard\n",
        "    \"https://www.gutenberg.org/cache/epub/55/pg55.txt\",\n",
        "  # CIENCIA FICCIÓN\n",
        "    # 4. A Journey to the Centre of the Earth\n",
        "    \"https://www.gutenberg.org/cache/epub/18857/pg18857.txt\",\n",
        "    # 5. The Time Machine\n",
        "    \"https://www.gutenberg.org/cache/epub/35/pg35.txt\",\n",
        "    # 6. The War of the Worlds\n",
        "    \"https://www.gutenberg.org/cache/epub/36/pg36.txt\",\n",
        "  # FANTASÍA\n",
        "    # 7. Alice's Adventures in Wonderland\n",
        "    \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",\n",
        "    # 8. Anne of Green Gables\n",
        "    \"https://www.gutenberg.org/cache/epub/45/pg45.txt\",\n",
        "    # 9. Peter Pan\n",
        "    \"https://www.gutenberg.org/cache/epub/16/pg16.txt\",\n",
        "  # ROMANCE\n",
        "    # 10. Price and Prejudice\n",
        "    \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\",\n",
        "    # 11. Romeo an Juliet\n",
        "    \"https://www.gutenberg.org/cache/epub/1513/pg1513.txt\",\n",
        "    # 12. A Room with a View\n",
        "    \"https://www.gutenberg.org/cache/epub/2641/pg2641.txt\",\n",
        "  # TERROR\n",
        "    # 13. Dracula\n",
        "    \"https://www.gutenberg.org/cache/epub/345/pg345.txt\",\n",
        "    # 14. Frankenstein\n",
        "    \"https://www.gutenberg.org/cache/epub/84/pg84.txt\",\n",
        "    # 15. The Strange Case of Dr. Jekyll and Mr. Hyde\n",
        "    \"https://www.gutenberg.org/cache/epub/43/pg43.txt\"\n",
        "    ]\n",
        "\n",
        "for url in GUTENBERG_URLS:\n",
        "    fname = url.split(\"/\")[-1]\n",
        "    out_path = Path(\"/content/raw_grueso\") / fname\n",
        "    if out_path.exists():\n",
        "        print(\"Ya existe:\", out_path)\n",
        "        continue\n",
        "    !wget -q \"{url}\" -O \"{out_path}\"\n",
        "    print(\"Descargado:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01kO0C8dm2VP",
        "outputId": "e3aee194-a40d-43e2-bed0-0482722cdc20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargado: /content/raw_grueso/pg2701.txt\n",
            "Descargado: /content/raw_grueso/pg74.txt\n",
            "Descargado: /content/raw_grueso/pg55.txt\n",
            "Descargado: /content/raw_grueso/pg18857.txt\n",
            "Descargado: /content/raw_grueso/pg35.txt\n",
            "Descargado: /content/raw_grueso/pg36.txt\n",
            "Descargado: /content/raw_grueso/pg11.txt\n",
            "Descargado: /content/raw_grueso/pg45.txt\n",
            "Descargado: /content/raw_grueso/pg16.txt\n",
            "Descargado: /content/raw_grueso/pg1342.txt\n",
            "Descargado: /content/raw_grueso/pg1513.txt\n",
            "Descargado: /content/raw_grueso/pg2641.txt\n",
            "Descargado: /content/raw_grueso/pg345.txt\n",
            "Descargado: /content/raw_grueso/pg84.txt\n",
            "Descargado: /content/raw_grueso/pg43.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despues de la descarga borre el principio y final a mano directo en el txt. habia texto de derechos de autor y eso"
      ],
      "metadata": {
        "id": "HxFn5jpnNHBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/raw_fino\", exist_ok=True)"
      ],
      "metadata": {
        "id": "nNipCXZ69zUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 3. LIMPIEZA AUTOMÁTICA DEL TEXTO DE GUTENBERG\n",
        "#    - Quitar licencia/cabecera/pie\n",
        "#    - Normalizar espacios\n",
        "#    - Dejar sólo el cuerpo de la obra\n",
        "# ============================================================\n",
        "\n",
        "def clean_gutenberg_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Limpia texto típico de Project Gutenberg:\n",
        "    - Remueve cabecera y pie de licencia.\n",
        "    - Quita múltiples líneas en blanco excesivas.\n",
        "    \"\"\"\n",
        "\n",
        "    #LIMPIAR A MANO\n",
        "\n",
        "    # Normalizar finales de línea\n",
        "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "\n",
        "    # Quitar dobles espacios\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "\n",
        "    # Colapsar bloques enormes de saltos de línea a como mucho 2\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Strip general\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_and_clean_all(input_dir=\"/content/raw_fino\") -> str:\n",
        "    \"\"\"Concatena todas las obras limpias en un único gran string.\"\"\"\n",
        "    corpus_parts = []\n",
        "    for path in sorted(Path(input_dir).glob(\"*.txt\")):\n",
        "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            raw = f.read()\n",
        "        clean = clean_gutenberg_text(raw)\n",
        "        paragraphs = split_into_paragraphs(clean)\n",
        "        chunks = make_chunks_from_paragraphs(paragraphs)\n",
        "        randomizados = random.sample(chunks, len(chunks))\n",
        "        usables = randomizados[:100]\n",
        "        corpus_parts.extend(usables)\n",
        "    return corpus_parts\n",
        "\n",
        "\n",
        "full_corpus = load_and_clean_all()\n",
        "print(\"Tamaño total corpus limpio (caracteres):\", len(full_corpus))\n",
        "\n",
        "full_corpus[1]  # vistazo rápido\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "6HuBNqgsm-KU",
        "outputId": "96fff785-afa8-4760-90b0-e50f4d1aba51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño total corpus limpio (caracteres): 134\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"PRIDE. I am Pride. I disdain to have any parents. I am like to Ovid's flea; I can creep into every corner of a wench; sometimes, like a perriwig, I sit upon her brow; or, like a fan of feathers, I kiss her lips; indeed, I do--what do I not? But, fie, what a scent is here! I'll not speak another word, except the ground were perfumed, and covered with cloth of arras. FAUSTUS. What art thou, the second? COVETOUSNESS. I am Covetousness, begotten of an old churl, in an old leathern bag: and, might I have my wish, I would desire that this house and all the people in it were turned to gold, that I might lock you up in my good chest: O, my sweet gold! FAUSTUS. What art thou, the third? WRATH. I am Wrath. I had neither father nor mother: I leapt out of a lion's mouth when I was scarce half-an-hour old; and ever since I have run up and down the world with this case[106] of rapiers, wounding myself when I had nobody to fight withal. I was born in hell; and look to it, for some of you shall be my father. FAUSTUS. What art thou, the fourth? ENVY. I am Envy, begotten of a chimney-sweeper and an oyster-wife. I cannot read, and therefore wish all books were burnt. I am lean with seeing others eat. O, that there would come a famine through all the world, that all might die, and I live alone! then thou shouldst see how fat I would be. But must thou sit, and I stand? come down, with a vengeance! FAUSTUS. Away, envious rascal!--What art thou, the fifth? GLUTTONY. Who I, sir? I am Gluttony. My parents are all dead, and the devil a penny they have left me, but a bare pension, and that is thirty meals a-day and ten bevers,[107]--a small trifle to suffice nature. O, I come of a royal parentage! my grandfather was a Gammon of Bacon, my grandmother a Hogshead of Claret-wine; my godfathers were these, Peter Pickle-herring and Martin Martlemas-beef; O, but my godmother, she was a jolly gentlewoman, and well-beloved in every good town and city; her name was Mistress Margery March-beer. Now, Faustus, thou hast heard all my progeny; wilt thou bid me to supper? FAUSTUS. No, I'll see thee hanged: thou wilt eat up all my victuals.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(full_corpus[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agWYZk_X3q8e",
        "outputId": "003ba8de-bf95-4703-9125-b797ed1d7a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 4. CHUNKS ESTILÍSTICOS\n",
        "#    - Dividir en párrafos y luego en trozos ~512-1024 tokens\n",
        "#    - Aquí aproximamos por número de palabras\n",
        "# ============================================================\n",
        "\n",
        "def split_into_paragraphs(text: str):\n",
        "    # Separar por líneas en blanco dobles\n",
        "    paras = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "    return paras\n",
        "\n",
        "\n",
        "def make_chunks_from_paragraphs(paragraphs, min_words=150, max_words=400):\n",
        "    \"\"\"\n",
        "    Junta párrafos hasta que haya entre min_words y max_words.\n",
        "    Devuelve una lista de strings (chunks).\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_words = []\n",
        "    current_len = 0\n",
        "\n",
        "    for para in paragraphs:\n",
        "        words = para.split()\n",
        "        if current_len + len(words) > max_words and current_words:\n",
        "            chunks.append(\" \".join(current_words))\n",
        "            current_words = []\n",
        "            current_len = 0\n",
        "        current_words.extend(words)\n",
        "        current_len += len(words)\n",
        "\n",
        "    # último\n",
        "    if current_words:\n",
        "        if len(current_words) >= min_words:\n",
        "            chunks.append(\" \".join(current_words))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "#paragraphs = split_into_paragraphs(full_corpus)\n",
        "#chunks = make_chunks_from_paragraphs(paragraphs, min_words=150, max_words=350)\n",
        "#\n",
        "#print(\"Párrafos:\", len(paragraphs))\n",
        "#print(\"Chunks finales:\", len(chunks))\n",
        "#print(\"Ejemplo de chunk:\\n\")\n",
        "#print(textwrap.fill(chunks[0][:800], width=90))\n"
      ],
      "metadata": {
        "id": "3x8pgDk1nHHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9cb1d94"
      },
      "source": [
        "# Task\n",
        "Shuffle the `chunks` list randomly, then split it into an 80% training set and a 20% validation set. Define a base path in Google Drive, create directories for the training and validation data, and save the respective chunks to text files in these directories. Finally, verify that the files have been saved correctly by listing the contents of the Google Drive directories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061e6e6b"
      },
      "source": [
        "## Shuffle and Split Chunks\n",
        "\n",
        "### Subtask:\n",
        "Shuffle the `chunks` list randomly and then split it into an 80% training set and a 20% validation set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc0af5b5"
      },
      "source": [
        "**Reasoning**:\n",
        "To shuffle and split the chunks, I will first import the `random` module, then shuffle the existing `chunks` list, and finally split it into training and validation sets based on the specified ratio.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6c4ac31"
      },
      "source": [
        "import random\n",
        "\n",
        "# Create a shuffled copy of the chunks list\n",
        "shuffled_chunks = random.sample(chunks, len(chunks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e4b397"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the chunks are split, I will define the Google Drive base path and create the directories for the training and validation datasets to prepare for saving the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2542b4ef",
        "outputId": "90c4f0e6-9082-4174-c40f-c2fc677d9cb0"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define Google Drive base path\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive/StoryWriter/Data\"\n",
        "\n",
        "# Define paths for training and validation directories\n",
        "TRAIN_DATA_DIR = os.path.join(DRIVE_BASE_PATH, \"redactor_train\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(TRAIN_DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Training data directory: {TRAIN_DATA_DIR}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data directory: /content/drive/MyDrive/StoryWriter/Data/redactor_train\n",
            "Validation data directory: /content/drive/MyDrive/StoryWriter/Data/casificator_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5dd143a"
      },
      "source": [
        "**Reasoning**:\n",
        "I will save the training and validation chunks to separate text files within their respective Google Drive directories. Each file will be prefixed with the `PROMPT_PREFIX` to maintain the desired format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcd83526",
        "outputId": "5a64a899-cec8-4abd-fec6-8af3931493c1"
      },
      "source": [
        "for i, chunk in enumerate(full_corpus):\n",
        "    file_path = os.path.join(\"/content/drive/MyDrive/StoryWriter/Data/Training_data/clasificator_train/negativos_finos\", f\"chunk_{i:04d}.txt\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(chunk)\n",
        "print(f\"Saved {len(full_corpus)} validation chunks to /content/drive/MyDrive/StoryWriter/Data/Training_data/clasificator_train/negativos_finos\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 134 validation chunks to /content/drive/MyDrive/StoryWriter/Data/Training_data/clasificator_train/negativos_finos\n"
          ]
        }
      ]
    }
  ]
}