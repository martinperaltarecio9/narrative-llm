{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3c6096f168d4f9eaf862862ed539814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa50a1fecf8240fe8648321d19b74034",
              "IPY_MODEL_2ae008dc97a34b7ba8d38864c4be243d",
              "IPY_MODEL_39f6525495414d36bd8d3413484390aa"
            ],
            "layout": "IPY_MODEL_febd1e5041bb41098342bdec4108733a"
          }
        },
        "aa50a1fecf8240fe8648321d19b74034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2bfff5643d244e5a1eb26f05b317081",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_79674f85a8954ba5a78c685dbe718599",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "2ae008dc97a34b7ba8d38864c4be243d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb6b6e9631334507a2074634c28e0ce1",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d3434bd837f458db3ada8c705a9b313",
            "value": 3
          }
        },
        "39f6525495414d36bd8d3413484390aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_552608029ad54d5aa5fd5e4e9df73ed9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a92c95a312c4313a10decca3d268129",
            "value": "‚Äá3/3‚Äá[01:28&lt;00:00,‚Äá37.12s/it]"
          }
        },
        "febd1e5041bb41098342bdec4108733a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2bfff5643d244e5a1eb26f05b317081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79674f85a8954ba5a78c685dbe718599": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb6b6e9631334507a2074634c28e0ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d3434bd837f458db3ada8c705a9b313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "552608029ad54d5aa5fd5e4e9df73ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a92c95a312c4313a10decca3d268129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpAqxPNWosxF"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 1. RUTAS Y PROMPTS\n",
        "# ============================================\n",
        "BASE_MODEL_DIR = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-7b-instruct-v0.3\"   # üîÅ CAMBIAR\n",
        "LORA_DIR       = \"/content/drive/MyDrive/StoryWriter/Modelo_FineTuning/mistral-finetuneado(lora)\"   # üîÅ CAMBIAR\n",
        "OUTPUT_DIR     = \"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data/mistral_finetune\"        # üîÅ CAMBIAR\n",
        "\n",
        "BASIC_PROMPT  = (\"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\")\n",
        "BETTER_PROMPT = (\"\"\"\n",
        "You are an expert writer imitating William Shakespeare.\n",
        "Write one single self-contained paragraph between 150 and 300 words in Early Modern English,\n",
        "in the style of Shakespeare‚Äôs plays and sonnets. The paragraph must be original, not copied,\n",
        "and should use iambic or quasi-iambic rhythm, archaic pronouns (thee, thou, thy), and\n",
        "elevated metaphors.\n",
        "Avoid copying any real Shakespeare sentences; the text must be entirely new.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 2. CARGAR TOKENIZER Y MODELOS\n",
        "# ============================================\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.model_max_length = 512\n",
        "\n",
        "# Modelo base\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_DIR,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        ")\n",
        "if device == \"cpu\":\n",
        "    model_base.to(device)\n",
        "model_base.eval()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518,
          "referenced_widgets": [
            "e3c6096f168d4f9eaf862862ed539814",
            "aa50a1fecf8240fe8648321d19b74034",
            "2ae008dc97a34b7ba8d38864c4be243d",
            "39f6525495414d36bd8d3413484390aa",
            "febd1e5041bb41098342bdec4108733a",
            "c2bfff5643d244e5a1eb26f05b317081",
            "79674f85a8954ba5a78c685dbe718599",
            "fb6b6e9631334507a2074634c28e0ce1",
            "0d3434bd837f458db3ada8c705a9b313",
            "552608029ad54d5aa5fd5e4e9df73ed9",
            "7a92c95a312c4313a10decca3d268129"
          ]
        },
        "id": "Bx69LXlEo6f9",
        "outputId": "b7d045f2-2135-49a0-e901-8246f4992751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3c6096f168d4f9eaf862862ed539814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32768, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "    (rotary_emb): MistralRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo + LoRA\n",
        "model_lora = PeftModel.from_pretrained(model_base, LORA_DIR)\n",
        "model_lora.eval()\n",
        "\n",
        "models = {\n",
        "    \"lora\": model_lora,\n",
        "}\n",
        "\n",
        "prompts = {\n",
        "    \"basic\": BASIC_PROMPT,\n",
        "    \"better\": BETTER_PROMPT,\n",
        "}\n"
      ],
      "metadata": {
        "id": "mE_sWAuo8U6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_base==model_lora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nbH8kTkEIE6",
        "outputId": "93f482d7-307e-49e6-c0f0-9ce8127199db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 3. FUNCI√ìN DE GENERACI√ìN\n",
        "# ============================================\n",
        "def generate_text(model, prompt, max_new_tokens=700, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.9,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "neANmfjfpDPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# 4. GENERAR 80 TEXTOS Y GUARDARLOS\n",
        "#    2 modelos √ó 2 prompts √ó 20 samples = 80\n",
        "# ============================================\n",
        "n_samples_per_combo = 20\n",
        "max_new_tokens = 700  # üîÅ ajust√° si quer√©s textos m√°s largos\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    for prompt_name, prompt in prompts.items():\n",
        "        print(f\"=== Generando para modelo={model_name}, prompt={prompt_name} ===\")\n",
        "        for i in range(n_samples_per_combo):\n",
        "            seed = 1000 + i  # cambia el seed para diversificar\n",
        "            text = generate_text(model, prompt, max_new_tokens=max_new_tokens, seed=seed)\n",
        "\n",
        "            filename = f\"{model_name}_{prompt_name}_{i:02d}.txt\"\n",
        "            out_path = os.path.join(OUTPUT_DIR, filename)\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(text)\n",
        "\n",
        "            print(f\"Guardado: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPi7JEHUpLVK",
        "outputId": "e39f13c2-82a1-4d18-d9e2-2db8f8c36630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Generando para modelo=lora, prompt=basic ===\n",
            "Guardado: lora_basic_00.txt\n",
            "Guardado: lora_basic_01.txt\n",
            "Guardado: lora_basic_02.txt\n",
            "Guardado: lora_basic_03.txt\n",
            "Guardado: lora_basic_04.txt\n",
            "Guardado: lora_basic_05.txt\n",
            "Guardado: lora_basic_06.txt\n",
            "Guardado: lora_basic_07.txt\n",
            "Guardado: lora_basic_08.txt\n",
            "Guardado: lora_basic_09.txt\n",
            "Guardado: lora_basic_10.txt\n",
            "Guardado: lora_basic_11.txt\n",
            "Guardado: lora_basic_12.txt\n",
            "Guardado: lora_basic_13.txt\n",
            "Guardado: lora_basic_14.txt\n",
            "Guardado: lora_basic_15.txt\n",
            "Guardado: lora_basic_16.txt\n",
            "Guardado: lora_basic_17.txt\n",
            "Guardado: lora_basic_18.txt\n",
            "Guardado: lora_basic_19.txt\n",
            "=== Generando para modelo=lora, prompt=better ===\n",
            "Guardado: lora_better_00.txt\n",
            "Guardado: lora_better_01.txt\n",
            "Guardado: lora_better_02.txt\n",
            "Guardado: lora_better_03.txt\n",
            "Guardado: lora_better_04.txt\n",
            "Guardado: lora_better_05.txt\n",
            "Guardado: lora_better_06.txt\n",
            "Guardado: lora_better_07.txt\n",
            "Guardado: lora_better_08.txt\n",
            "Guardado: lora_better_09.txt\n",
            "Guardado: lora_better_10.txt\n",
            "Guardado: lora_better_11.txt\n",
            "Guardado: lora_better_12.txt\n",
            "Guardado: lora_better_13.txt\n",
            "Guardado: lora_better_14.txt\n",
            "Guardado: lora_better_15.txt\n",
            "Guardado: lora_better_16.txt\n",
            "Guardado: lora_better_17.txt\n",
            "Guardado: lora_better_18.txt\n",
            "Guardado: lora_better_19.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASIC_PROMPT  = (\"\"\"\n",
        "    Write a single paragraph between 150 and 300 words in the style of\n",
        "    Shakespeare's stories. The paragraph must be original,\n",
        "    not copied, and self-contained.\n",
        "    \"\"\")\n",
        "BETTER_PROMPT = (\"\"\"\n",
        "You are an expert writer imitating William Shakespeare.\n",
        "\n",
        "Write one single self-contained paragraph between 150 and 300 words in Early Modern English,\n",
        "in the style of Shakespeare‚Äôs plays and sonnets. The paragraph must be original, not copied,\n",
        "and should use iambic or quasi-iambic rhythm, archaic pronouns (thee, thou, thy), and\n",
        "elevated metaphors.\n",
        "\n",
        "Avoid copying any real Shakespeare sentences; the text must be entirely new.\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "VXzm-6IF2-xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(BETTER_PROMPT))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsGusRqRBKVu",
        "outputId": "f97d0a56-9064-4ac1-d435-9311ead84d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT_DIR = \"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data/mistral_finetune_prompt_pro\"   # carpeta donde est√°n los .txt\n",
        "\n",
        "\n",
        "for path in Path(INPUT_DIR).glob(\"*.txt\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # borrar el prompt si aparece al comienzo\n",
        "    cleaned = text[426:]\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(cleaned)\n"
      ],
      "metadata": {
        "id": "tZZkZs7z1qgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "INPUT_DIR = \"/content/drive/MyDrive/StoryWriter/Data/Benchmark_data/mistral_finetune\"   # carpeta donde est√°n los .txt\n",
        "\n",
        "\n",
        "for path in Path(INPUT_DIR).glob(\"*.txt\"):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # borrar el prompt si aparece al comienzo\n",
        "    cleaned = text[171:]\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(cleaned)\n"
      ],
      "metadata": {
        "id": "2z7jPDBr1W7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}